{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d41ea00991cca0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLMFactory\n",
    "\n",
    "> We recommend use pne.chat() to create your LLM application after v1.16.0. pne.ChatOpenAI, pne.ZhipuAI and pne.Qianfan... will be deprecated in the future. Now we use LLMFactory to create any LLM.\n",
    "\n",
    "## Features\n",
    "\n",
    "- LLMFactory is a factory class to create LLM model. \n",
    "- It is the basic component of the pne.chat model driver for creating LLM models.\n",
    "- It integrates the ability of [litellm](https://github.com/BerriAI/litellm). It means you can call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). Now let's take a look at how to use it.\n",
    "\n",
    "The following example show how to create an OpenAI model and chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:26:47.768667Z",
     "start_time": "2024-04-29T15:26:39.550739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pne\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "model = pne.LLMFactory.build(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    model_config={\n",
    "        \"temperature\": 0.5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55f4cf323489372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:26:56.351688Z",
     "start_time": "2024-04-29T15:26:53.084169Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "resp: str = model(\"hello, how are you?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55145e3e8e15f8d4",
   "metadata": {},
   "source": "You can also initialize the model with the following code:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6451ecee95b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pne\n",
    "\n",
    "pne.LLMFactory.build(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    model_config={\n",
    "        \"temperature\": 0.5,\n",
    "        \"api_key\": \"your-api_key\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e9577170dabdf",
   "metadata": {},
   "source": [
    "## Use OpenAI Proxy\n",
    "The following example show how to use [AIGCAPI proxy](https://aigcapi.io/) to call OpenAI gpt-4-turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2176e911b8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pne\n",
    "\n",
    "model = pne.LLMFactory.build(\n",
    "    model_name=\"gpt-4-turbo\",\n",
    "    model_config={\n",
    "        \"api_key\": \"your-api_key\",\n",
    "        \"api_base\": \"https://api.aigcapi.io\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941b48fb56b7f18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What's different between LLMFactory and pne.chat() ? \n",
    "\n",
    "LLMFactory is the basic component of the pne.chat model driver for creating LLM models. So at most time, you don't need to use LLMFactory directly. If you are developing a chatbot, you need to use pne.chat() to chat and make a structure of response. Eg:\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str] = Field(description=\"All provinces in China\")\n",
    "\n",
    "\n",
    "response: LLMResponse = pne.chat(\n",
    "    messages=\"Please tell me all provinces in China.\",\n",
    "    output_schema=LLMResponse,\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "print(response.provinces)\n",
    "```\n",
    "\n",
    "pne.chat() covers 90% of development scenarios, so we recommend using pne.chat() if there are no special needs.\n",
    "\n",
    "- If you want to use OpenAI, HuggingFace, Bedrock, Azure, Cohere, Anthropic, Ollama, Sagemaker, Replicate, etc., please use pne.chat() directly. It's a simple and easy way to chat.\n",
    "- If you want to use a custom LLM model, please read [CustomLLM](https://undertone0809.github.io/promptulate/#/modules/llm/custom_llm?id=custom-llm)\n",
    "\n",
    "## Why show LLMFactory here?\n",
    "\n",
    "We just want you know the basic component of the pne.chat model driver for creating LLM models. If you are a developer, you can learn something from this design. If you are a user, you can know how pne.chat works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
